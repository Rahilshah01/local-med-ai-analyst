{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f784101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Local AI Analyst ---\n",
      "Answer: There is no mention of the dose of Suzetrigine being administered in this context. The information provided only discusses the results and outcomes of the studies, without mentioning the dosage of the medication.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc # Garbage collection to free up RAM\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 1. Loading PDF\n",
    "loader = PyPDFLoader(\"suzetrigine_study.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "# 2. Splitting into smaller chunks (less RAM usage per chunk)\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# 3. Using an In-Memory search (instead of ChromaDB files)\n",
    "# This is faster and avoids disk I/O crashes\n",
    "embeddings = OllamaEmbeddings(model=\"phi3\")\n",
    "vectorstore = DocArrayInMemorySearch.from_documents(splits, embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# 4. Using the llama3 Model\n",
    "model = ChatOllama(model=\"llama3\")\n",
    "\n",
    "# 5. Defining the Prompt\n",
    "template = \"\"\"Answer based ONLY on the context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# 6. Building the Chain\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": lambda x: x}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 7. Execute and immediately clear memory\n",
    "try:\n",
    "    print(\"--- Local AI Analyst ---\")\n",
    "    response = rag_chain.invoke(\"What dose of Suzetrigine was administered?\")\n",
    "    print(f\"Answer: {response}\")\n",
    "finally:\n",
    "    gc.collect() # Force Python to clean up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440ccdd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
